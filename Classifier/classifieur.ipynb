{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13717f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score,\n",
    "    f1_score, accuracy_score, classification_report\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve, validation_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# NLTK downloads (Ã  lancer une seule fois)\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62982f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_MAPPING = {\n",
    "    # Fiction\n",
    "    'fiction': 'Fiction',\n",
    "    'english fiction': 'Fiction',\n",
    "    'american fiction': 'Fiction',\n",
    "    'detective and mystery stories': 'Fiction',\n",
    "    '\"childrens stories\"': 'Fiction',\n",
    "\n",
    "    # Juvenile\n",
    "    'juvenile fiction': 'Juvenile',\n",
    "    'juvenile nonfiction': 'Juvenile',\n",
    "    'young adult nonfiction': 'Juvenile',\n",
    "    'childrens stories': 'Juvenile',\n",
    "\n",
    "    # Religion & Spirituality\n",
    "    'religion': 'Religion & Spirituality',\n",
    "    'spirit': 'Religion & Spirituality',\n",
    "    'bibles': 'Religion & Spirituality',\n",
    "    'bible': 'Religion & Spirituality',\n",
    "\n",
    "    # Biography\n",
    "    'biography': 'Biography',\n",
    "    'autobiography': 'Biography',\n",
    "    'true crime': 'Biography',\n",
    "\n",
    "    # Health & Wellness\n",
    "    'body, mind': 'Health & Wellness',\n",
    "    'health': 'Health & Wellness',\n",
    "    'fitness': 'Health & Wellness',\n",
    "    'relationships': 'Health & Wellness',\n",
    "    'family': 'Health & Wellness',\n",
    "    'self-help': 'Health & Wellness',\n",
    "    'medical': 'Health & Wellness',\n",
    "\n",
    "    # Business & Economics\n",
    "    'business': 'Business & Economics',\n",
    "    'economics': 'Business & Economics',\n",
    "\n",
    "    # Social Sciences\n",
    "    'social science': 'Social Sciences',\n",
    "    'political science': 'Social Sciences',\n",
    "    'philosophy': 'Social Sciences',\n",
    "    'psychology': 'Social Sciences',\n",
    "    'disciplines': 'Social Sciences',\n",
    "    'law': 'Social Sciences',\n",
    "\n",
    "    # History\n",
    "    'history': 'History',\n",
    "    'great britain': 'History',\n",
    "\n",
    "    # Computers & Tech\n",
    "    'computers': 'Computers & Tech',\n",
    "    'technology': 'Computers & Tech',\n",
    "    'engineering': 'Computers & Tech',\n",
    "\n",
    "    # Science & Nature\n",
    "    'science': 'Science & Nature',\n",
    "    'nature': 'Science & Nature',\n",
    "    'mathematics': 'Science & Nature',\n",
    "    'animals': 'Science & Nature',\n",
    "\n",
    "    # Cooking\n",
    "    'cooking': 'Cooking',\n",
    "\n",
    "    # Recreation & Sports\n",
    "    'recreation': 'Recreation & Sports',\n",
    "    'sports': 'Recreation & Sports',\n",
    "    'games': 'Recreation & Sports',\n",
    "    'hobbies': 'Recreation & Sports',\n",
    "    'crafts': 'Recreation & Sports',\n",
    "\n",
    "    # Education & Language\n",
    "    'language arts': 'Education & Language',\n",
    "    'education': 'Education & Language',\n",
    "    'study aids': 'Education & Language',\n",
    "    'foreign language study': 'Education & Language',\n",
    "\n",
    "    # Arts & Entertainment\n",
    "    'art': 'Arts & Entertainment',\n",
    "    'music': 'Arts & Entertainment',\n",
    "    'performing arts': 'Arts & Entertainment',\n",
    "    'photography': 'Arts & Entertainment',\n",
    "    'humor': 'Arts & Entertainment',\n",
    "    'architecture': 'Arts & Entertainment',\n",
    "    'design': 'Arts & Entertainment',\n",
    "    'antiques': 'Arts & Entertainment',\n",
    "    'collectibles': 'Arts & Entertainment',\n",
    "\n",
    "    # Travel & Lifestyle\n",
    "    'travel': 'Travel & Lifestyle',\n",
    "    'home': 'Travel & Lifestyle',\n",
    "    'house': 'Travel & Lifestyle',\n",
    "    'gardening': 'Travel & Lifestyle',\n",
    "    'pets': 'Travel & Lifestyle',\n",
    "    'activities': 'Travel & Lifestyle',\n",
    "    'transportation': 'Travel & Lifestyle',\n",
    "\n",
    "    # Reference\n",
    "    'reference': 'Reference',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelClassifier_TFIDF_NAIVEBAYES:\n",
    "    def __init__(self, csv_path, model_path=None):\n",
    "        self.csv_path = csv_path\n",
    "        self.model_path = model_path or \"models/ModelClassifier_TFIDF_NAIVEBAYES.joblib\"\n",
    "        self.dataset = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.model = LogisticRegression()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('classifier', LogisticRegression())\n",
    "        ])\n",
    "        self.param_grid = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.75],\n",
    "            'vectorizer__min_df': [1],\n",
    "            'classifier__C': [0.1, 1],\n",
    "            'classifier__solver': ['saga'],\n",
    "            'classifier__penalty': ['l2'],\n",
    "            'classifier__class_weight': ['balanced'],\n",
    "            'classifier__max_iter': [500]\n",
    "        }\n",
    "\n",
    "    def load_data(self):\n",
    "        self.dataset = pd.read_csv(self.csv_path)\n",
    "        self.dataset.drop_duplicates(inplace=True)\n",
    "        self.dataset.dropna(inplace=True)\n",
    "\n",
    "    def clean_text(self, sentence):\n",
    "        sentence = sentence.strip().lower()\n",
    "        sentence = re.sub(r\"[^a-z\\s]\", '', sentence)\n",
    "        tokens = word_tokenize(sentence)\n",
    "        filtered = [w for w in tokens if w not in self.stop_words]\n",
    "        lemmatized = [self.lemmatizer.lemmatize(w, pos='v') for w in filtered]\n",
    "        return ' '.join(lemmatized)\n",
    "\n",
    "    def clean_target(self, category):\n",
    "        if pd.isna(category) or not isinstance(category, str):\n",
    "            return [\"unknown\"]\n",
    "        else:\n",
    "            category = re.sub(r\"[\\[\\]']+\", \"\", category)\n",
    "            return [g.strip().lower() for g in category.split(\"&\")]\n",
    "\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.dataset['clean_description'] = self.dataset['description'].apply(self.clean_text)\n",
    "        self.dataset['clean_categories'] = self.dataset['categories'].apply(self.clean_target)\n",
    "        self.dataset = self.dataset.explode('clean_categories')\n",
    "\n",
    "        self.dataset['clean_categories'] = self.dataset['clean_categories'].apply(lambda x: CATEGORY_MAPPING.get(x.lower(), x))\n",
    "        # print(f'Liste des 30 categories les plus frequentes : {self.dataset['clean_categories'].value_counts().head(30)}')\n",
    "        \n",
    "        category_counts = self.dataset['clean_categories'].value_counts()\n",
    "        valid_categories = category_counts[category_counts >= 100].index\n",
    "        self.dataset = self.dataset[self.dataset['clean_categories'].isin(valid_categories)]\n",
    "        print(f'Liste des 30 categories les plus frequentes : {self.dataset['clean_categories'].value_counts().head(30)}')\n",
    "        self.dataset['publishedDate'] = pd.to_datetime(self.dataset['publishedDate'], format='%Y-%M-%d', errors='coerce')\n",
    "        self.dataset =  self.dataset.dropna()\n",
    "        self.dataset['publishedDate'] = self.dataset['publishedDate'].dt.strftime('%Y')\n",
    "        # self.dataset['date'] =  self.dataset['publishedDate'].dt.year\n",
    "        self.dataset = self.dataset[['clean_description', 'publishedDate', 'clean_categories']]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.X = self.dataset['clean_description']\n",
    "        self.y = self.dataset['clean_categories']\n",
    "\n",
    "    def train_test_split(self, test_size=0.2, valid_size = 0.05, random_state=42):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.X_temp, self.X_valid, self.y_temp, self.y_valid = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X_temp, self.y_temp, test_size=valid_size, random_state=random_state)\n",
    "        self.y_train = self.label_encoder.fit_transform(self.y_train)\n",
    "        self.y_test = self.label_encoder.transform(self.y_test)\n",
    "        self.y_valid = self.label_encoder.transform(self.y_valid)\n",
    "\n",
    "    def train(self):\n",
    "        X_train_vec = self.vectorizer.fit_transform(self.X_train)\n",
    "        self.model.fit(X_train_vec, self.y_train)\n",
    "\n",
    "    def evaluate(self):\n",
    "        if hasattr(self, 'best_model'):\n",
    "            y_pred = self.best_model.predict(self.X_test)\n",
    "        else:\n",
    "            X_test_vec = self.vectorizer.transform(self.X_test)\n",
    "            y_pred = self.model.predict(X_test_vec)\n",
    "\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "\n",
    "    def grid_search(self, param_grid=None, cv=5, scoring='accuracy'):\n",
    "        print(\"Running Grid Search...\")\n",
    "        if param_grid is None:\n",
    "            param_grid = self.param_grid\n",
    "\n",
    "        grid = GridSearchCV(self.pipeline, param_grid, cv=cv, scoring=scoring, n_jobs=-2)\n",
    "        grid.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(f\"Best Parameters: {grid.best_params_}\")\n",
    "        print(f\"Best CV Score: {grid.best_score_:.4f}\")\n",
    "\n",
    "        self.best_model = grid.best_estimator_\n",
    "\n",
    "    def evaluate_best_model(self):\n",
    "        if hasattr(self, 'best_model'):\n",
    "            y_pred = self.best_model.predict(self.X_test)\n",
    "            print(classification_report(self.y_test, y_pred))\n",
    "        else:\n",
    "            print(\"Best model not found. Run grid_search() first.\")\n",
    "\n",
    "    def predict_text(self, text, return_proba=False):\n",
    "        cleaned = self.clean_text(text)\n",
    "\n",
    "        if hasattr(self, 'best_model'):\n",
    "            vectorizer = self.best_model.named_steps['vectorizer']\n",
    "            model = self.best_model.named_steps['classifier']\n",
    "        else:\n",
    "            vectorizer = self.vectorizer\n",
    "            model = self.model\n",
    "\n",
    "        X_input = vectorizer.transform([cleaned])\n",
    "        prediction = model.predict(X_input)[0]\n",
    "\n",
    "        if return_proba and hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_input)[0]\n",
    "            class_proba = dict(zip(model.classes_, proba))\n",
    "            return prediction, class_proba\n",
    "\n",
    "        label = self.label_encoder.inverse_transform([prediction])[0]\n",
    "        return label\n",
    "\n",
    "    def save_model(self):\n",
    "        joblib.dump(self.best_model, self.model_path)\n",
    "        print(f\"ğ¾ ModÃ¨le sauvegardÃ© dans {self.model_path}\")\n",
    "\n",
    "    def load_or_train(self, X, y, force_retrain=False):\n",
    "        if os.path.exists(self.model_path) and not force_retrain:\n",
    "            print(f\"ğ Chargement du modÃ¨le depuis {self.model_path}\")\n",
    "            self.best_model = joblib.load(self.model_path)\n",
    "        else:\n",
    "            print(\"ğ ï¸  EntraÃ®nement du modÃ¨le avec GridSearchCV...\")\n",
    "            grid_search = GridSearchCV(self.pipeline, self.param_grid, cv=10, scoring='accuracy', n_jobs=-2, verbose=1)\n",
    "            grid_search.fit(X, y)\n",
    "            self.best_model = grid_search.best_estimator_\n",
    "            self.save_model()\n",
    "            print(\"â ModÃ¨le entraÃ®nÃ© et sauvegardÃ©.\")\n",
    "\n",
    "    def plot_performance(self, param_name=None, param_range=None, cv=5):\n",
    "        if not hasattr(self, 'X_train') or not hasattr(self, 'y_train'):\n",
    "            raise ValueError(\"DonnÃ©es d'entraÃ®nement manquantes.\")\n",
    "\n",
    "        pipeline = self.best_model if hasattr(self, 'best_model') else self.pipeline\n",
    "        X = self.X_train\n",
    "        y = self.y_train\n",
    "\n",
    "        # Courbe d'apprentissage\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            pipeline, X, y, cv=cv, n_jobs=-2, scoring='accuracy',\n",
    "            train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "        )\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label=\"EntraÃ®nement\")\n",
    "        plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label=\"Validation\")\n",
    "        plt.title(\"Courbe d'apprentissage\")\n",
    "        plt.xlabel(\"Taille d'entraÃ®nement\")\n",
    "        plt.ylabel(\"Exactitude\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Courbe de validation\n",
    "        if param_name and param_range is not None:\n",
    "            train_scores, valid_scores = validation_curve(\n",
    "                pipeline, X, y, param_name=param_name,\n",
    "                param_range=param_range, cv=cv, scoring=\"accuracy\", n_jobs=-2\n",
    "            )\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(param_range, np.mean(train_scores, axis=1), 'o-', label=\"EntraÃ®nement\")\n",
    "            plt.plot(param_range, np.mean(valid_scores, axis=1), 'o-', label=\"Validation\")\n",
    "            plt.title(f\"Courbe de validation pour {param_name}\")\n",
    "            plt.xlabel(param_name)\n",
    "            plt.ylabel(\"Exactitude\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Matrice de confusion et mÃ©triques\n",
    "        if hasattr(self, 'X_test') and hasattr(self, 'y_test'):\n",
    "            y_pred = pipeline.predict(self.X_test)\n",
    "            cm = confusion_matrix(self.y_test, y_pred)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "            fig, ax = plt.subplots(figsize=(6, 5))\n",
    "            disp.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)\n",
    "            plt.title(\"Matrice de confusion\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            precision = precision_score(self.y_test, y_pred, average='weighted', zero_division=0)\n",
    "            recall = recall_score(self.y_test, y_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(self.y_test, y_pred, average='weighted', zero_division=0)\n",
    "            accuracy = accuracy_score(self.y_test, y_pred)\n",
    "\n",
    "            print(\"ğ **MÃ©triques de classification sur les donnÃ©es de test**\")\n",
    "            print(f\" - Accuracy  : {accuracy:.6f}\")\n",
    "            print(f\" - Precision : {precision:.6f}\")\n",
    "            print(f\" - Recall    : {recall:.6f}\")\n",
    "            print(f\" - F1-Score  : {f1:.6f}\")\n",
    "        else:\n",
    "            print(\"â ï¸ DonnÃ©es de test non disponibles pour l'Ã©valuation finale.\")\n",
    "\n",
    "    def run_all(self, test_size=0.2, valid_size = 0.05, force_retrain=False):\n",
    "        print(\"ğ¥ Chargement des donnÃ©es...\")\n",
    "        self.load_data()\n",
    "\n",
    "        # self.dataset = self.dataset.head(1000)  # Limiter Ã  1000 lignes pour les tests\n",
    "\n",
    "        print(\"ğ§¹ Nettoyage et prÃ©traitement...\")\n",
    "        self.preprocess()\n",
    "\n",
    "        print(\"ğ ï¸ PrÃ©paration des variables X et y...\")\n",
    "        self.prepare_data()\n",
    "\n",
    "        print(\"âï¸ SÃ©paration en jeu d'entraÃ®nement et de test...\")\n",
    "        self.train_test_split(test_size=test_size, valid_size=valid_size)\n",
    "\n",
    "        print(\"ğ Chargement ou entraÃ®nement du modÃ¨le...\")\n",
    "        self.load_or_train(self.X_train, self.y_train, force_retrain=force_retrain)\n",
    "\n",
    "        print(\"ğ§ª Ãvaluation sur le jeu de test...\")\n",
    "        self.evaluate()\n",
    "\n",
    "        print(\"ğ Affichage des courbes de performance...\")\n",
    "        self.plot_performance()\n",
    "\n",
    "        print(\"ğ Fin du pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "985022bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğ¥ Chargement des donnÃ©es...\n",
      "ğ§¹ Nettoyage et prÃ©traitement...\n",
      "Liste des 30 categories les plus frequentes : clean_categories\n",
      "Fiction                    11148\n",
      "Biography                   4792\n",
      "Juvenile                    4386\n",
      "Religion & Spirituality     3584\n",
      "Health & Wellness           3561\n",
      "Social Sciences             2743\n",
      "History                     2470\n",
      "Business & Economics        2120\n",
      "Arts & Entertainment        1745\n",
      "Recreation & Sports         1635\n",
      "Computers & Tech            1415\n",
      "Science & Nature            1282\n",
      "Education & Language        1027\n",
      "Travel & Lifestyle           942\n",
      "Cooking                      600\n",
      "young adult fiction          442\n",
      "literary criticism           411\n",
      "poetry                       379\n",
      "comics                       300\n",
      "graphic novels               300\n",
      "Reference                    263\n",
      "drama                        234\n",
      "literary collections         192\n",
      "Name: count, dtype: int64\n",
      "ğ ï¸ PrÃ©paration des variables X et y...\n",
      "âï¸ SÃ©paration en jeu d'entraÃ®nement et de test...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7, 14,  6, ..., 15,  7,  6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23130,)\n",
      "ğ Chargement ou entraÃ®nement du modÃ¨le...\n",
      "ğ ï¸  EntraÃ®nement du modÃ¨le avec GridSearchCV...\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m modelLR \u001b[38;5;241m=\u001b[39m ModelClassifier_TFIDF_NAIVEBAYES(csv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/books_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodelLR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_retrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 231\u001b[0m, in \u001b[0;36mModelClassifier_TFIDF_NAIVEBAYES.run_all\u001b[1;34m(self, test_size, valid_size, force_retrain)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39mtest_size, valid_size\u001b[38;5;241m=\u001b[39mvalid_size)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğ Chargement ou entraÃ®nement du modÃ¨le...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_or_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_retrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_retrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğ§ª Ãvaluation sur le jeu de test...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "Cell \u001b[1;32mIn[28], line 145\u001b[0m, in \u001b[0;36mModelClassifier_TFIDF_NAIVEBAYES.load_or_train\u001b[1;34m(self, X, y, force_retrain)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğ ï¸  EntraÃ®nement du modÃ¨le avec GridSearchCV...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 145\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[1;32mc:\\Users\\calme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\calme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\calme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\calme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\calme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\calme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\calme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\calme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelLR = ModelClassifier_TFIDF_NAIVEBAYES(csv_path=\"data/books_data.csv\")\n",
    "modelLR.run_all(test_size=0.2, valid_size=0.05, force_retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5161492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_text = modelLR.predict_text('Long ago, legions of creatures called Kaiju came out of the sea, bringing war. To fight the Kaiju, humanity creates giant robots called Jaegers, designed to be driven by two humans locked together in a neutral bridge. On the other hand, even the Jaegers are not enough to defeat the Kaiju, and humanity is on the verge of defeat. The last hope now rests on an ex-pilot, a trainee without experience and an old obsolete Jaeger.')\n",
    "run_text\n",
    "\n",
    "decoded_preds = modelLR.label_encoder.inverse_transform([run_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "698ec576",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_text = modelLR.predict_text('Duke Leto Atreides of House Atreides, ruler of the ocean world Caladan, is assigned by the Padishah Emperor Shaddam IV to serve as fief ruler of the planet Arrakis. Although Arrakis is a harsh and inhospitable desert planet, it is of enormous importance because it is the only planetary source of melange, or the \"spice\", a unique and incredibly valuable substance that extends human youth, vitality and lifespan. It is also through the consumption of spice that Spacing Guild Navigators are able to effect safe interstellar travel through a limited ability to see into the future. The Emperor is jealous of the Duke s rising popularity in the Landsraad, the council of Great Houses, and sees House Atreides as a potential rival and threat. He conspires with House Harkonnen, the former stewards of Arrakis and the longstanding enemies of the Atreides, to destroy Leto and his family after their arrival. Leto is aware his assignment is a trap of some kind, but is compelled to obey the Emperor s orders anyway.')\n",
    "run_text\n",
    "\n",
    "decoded_preds = modelLR.label_encoder.inverse_transform([run_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b03129ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_text = modelLR.predict_text('I Know Why the Caged Bird Sings follows Marguerite s (called My or Maya by her brother) life from the age of three to seventeen and the struggles she facesâparticularly with racism and self-affirmationâin the Southern United States. Abandoned by their parents, Maya and her older brother Bailey are sent to live with their paternal grandmother (Momma) and disabled uncle (Uncle Willie) in Stamps, Arkansas. Maya and Bailey are haunted by their parents abandonment throughout the bookâthey travel alone and are labeled like baggage.')\n",
    "decoded_preds = modelLR.label_encoder.inverse_transform([run_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "060a4d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['juvenile fiction'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7a61f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_description</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>category_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>twentyfive years ago height counterculture mov...</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>twentyfive years ago height counterculture mov...</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bismarck perhaps famous notorious warship ever...</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>lebron jam sixfooteight gift basketball heaven...</td>\n",
       "      <td>2003</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>lebron jam sixfooteight gift basketball heaven...</td>\n",
       "      <td>2003</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212374</th>\n",
       "      <td>want lose weight diet doesnt seem work ive try...</td>\n",
       "      <td>2005</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212374</th>\n",
       "      <td>want lose weight diet doesnt seem work ive try...</td>\n",
       "      <td>2005</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212394</th>\n",
       "      <td>grace father believe science build daughter do...</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212399</th>\n",
       "      <td>school trip ellis island dominick avaro tenyea...</td>\n",
       "      <td>2000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212402</th>\n",
       "      <td>alexli tandem sell autograph business hunt nam...</td>\n",
       "      <td>2003</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26299 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_description publishedDate  \\\n",
       "31      twentyfive years ago height counterculture mov...          2012   \n",
       "31      twentyfive years ago height counterculture mov...          2012   \n",
       "33      bismarck perhaps famous notorious warship ever...          2018   \n",
       "45      lebron jam sixfooteight gift basketball heaven...          2003   \n",
       "45      lebron jam sixfooteight gift basketball heaven...          2003   \n",
       "...                                                   ...           ...   \n",
       "212374  want lose weight diet doesnt seem work ive try...          2005   \n",
       "212374  want lose weight diet doesnt seem work ive try...          2005   \n",
       "212394  grace father believe science build daughter do...          2015   \n",
       "212399  school trip ellis island dominick avaro tenyea...          2000   \n",
       "212402  alexli tandem sell autograph business hunt nam...          2003   \n",
       "\n",
       "        category_encoded  \n",
       "31                     1  \n",
       "31                     0  \n",
       "33                    12  \n",
       "45                    28  \n",
       "45                    21  \n",
       "...                  ...  \n",
       "212374                11  \n",
       "212374                10  \n",
       "212394                 9  \n",
       "212399                13  \n",
       "212402                 9  \n",
       "\n",
       "[26299 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLR.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60a6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
